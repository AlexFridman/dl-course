{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seminar 6: Hacking R-CNN\n",
    "====================\n",
    "\n",
    "**WARNING: This seminar was not thoroughly tested so expect weird bugs. Please report if you found one!**\n",
    "\n",
    "In this assignment, you will hack the existing code for object detection (https://github.com/rbgirshick/py-faster-rcnn) in order to make it usable with **Theano**-based models.\n",
    "\n",
    "Originally, Ross Girshick uses [Caffe](http://caffe.berkeleyvision.org/) as a backend for deep learning. This is not very good for us as Caffe is substantially different from Theano, and one cannot simply replace one framework with the other. Luckily, the training and the testing procedures are built on top of the [Python interface](http://caffe.berkeleyvision.org/tutorial/interfaces.html) (instead of the Caffe's native CLI+protobufs combo) which makes it possible to locate and exterminate Caffe-contaminated parts and fill the gaps with appropriate wrappers around Theano machinery.\n",
    "\n",
    "While it sounds like an easy task, this surgery still requires good familiarity with the RCNN's internals, so you will have to go through the Rob's slides and the code and make sure that you understand what is happening in each of the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful imports that we are going to be needing throughout the seminar.\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNN introduction\n",
    "--------------------\n",
    "\n",
    "In a nutshell, R-CNN is just a common image recognition neural network applied to patches that obtained by rescaling rectangular regions of a bigger scene. Those regions are usually computed externally using, for example, [Selective Search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/). The overall pipeline is best described by the following slide from the Girshick's [awesome presentation](http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf):\n",
    "<img src=\"notebook/img/rcnn_slide.jpg\" width=600px><img>\n",
    "\n",
    "You are encouraged to read the [relevant paper](http://www.cs.berkeley.edu/~rbg/papers/pami/rcnn_pami.pdf) to get deeper understanding of how the system works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the code\n",
    "--------------------\n",
    "\n",
    "### Cython modules\n",
    "\n",
    "First, we need to build Cython modules that come with the original Faster R-CNN package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ./lib; make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PASCAL VOC 2007\n",
    "\n",
    "In our experiments, we are going to use [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) so we download training, validation and test data as well as VOCdevkit (**this may take a while!**). We are assuming you have the `wget` utility installed on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you already have the dataset on your machine just put the absolute path (root path - w/o VOCdevkit) \n",
    "# into the following variable:\n",
    "PASCALVOC2007_PATH=''\n",
    "\n",
    "if not PASCALVOC2007_PATH:\n",
    "    # You can change the download target path.\n",
    "    PASCALVOC2007_PATH='./downloads'\n",
    "    \n",
    "    !mkdir -p {PASCALVOC2007_PATH}\n",
    "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar {PASCALVOC2007_PATH}\n",
    "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar {PASCALVOC2007_PATH}\n",
    "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar {PASCALVOC2007_PATH}\n",
    "    \n",
    "    !cd {PASCALVOC2007_PATH}; tar xvf VOCtrainval_06-Nov-2007.tar\n",
    "    !cd {PASCALVOC2007_PATH}; tar xvf VOCtest_06-Nov-2007.tar\n",
    "    !cd {PASCALVOC2007_PATH}; tar xvf VOCdevkit_08-Jun-2007.tar\n",
    "    \n",
    "# We symlink the dataset path to the folder that is expected by the R-CNN code.\n",
    "VOCDEVKIT_PATH = os.path.join(PASCALVOC2007_PATH, 'VOCdevkit')\n",
    "!ln -f -s {VOCDEVKIT_PATH} ./data/VOCdevkit2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposals\n",
    "\n",
    "Next, you need to fetch some external region proposals. The original code uses Selective Search by default. You can download and setup this data by running the cell below. It is very time-consuming, so go have some coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, you can ask people who have already downloaded the data to share it. \n",
    "# Then you can just put it in the ./data folder and skip this step.\n",
    "!data/scripts/fetch_selective_search_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other option would be to use precomuted [RPN proposals](https://github.com/ShaoqingRen/faster_rcnn#resources). Fetch them from [here](https://yadi.sk/d/ENCF0DTXr323P) and unpack to the `./data` folder. The advantage of using them is that you only need **300** entities per image (instead of **2000**) to get a good performance.\n",
    "\n",
    "### Pretrained base models\n",
    "\n",
    "Finally, you need to download one of the pretrained NNs that we will use a starting point for our object detection pipeline. As we are using Lasagne, it's a good idea to check out [Lasagne's ModelZoo](https://github.com/Lasagne/Recipes/tree/master/modelzoo). We suggest not to go crazy and use a moderately-sized model like `caffe_reference` (or even shallower). We all want to set the new state-of-the-art for Object Detection, but let's start small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your modules\n",
    "--------------------\n",
    "\n",
    "The most interesting part of the assignment is, of course, the custom model injection. As we already mentioned, the original code uses Caffe. We (the TAs) did our best to get rid of this nasty dependency. **Now, spend some time comparing the code provided by us and the [Faster R-CNN repo](https://github.com/rbgirshick/py-faster-rcnn).** The code makes heavy use of the configuration files available by calling `from fast_rcnn.config import cfg`. The `cfg` contains various parameters that govern the behaviour of the whole system. The standard configuration is defined in `./lib/fast_rcnn/config.py`. In order to override it use `./experiments/cfgs/rcnn.yml`.\n",
    "\n",
    "You'll have to\n",
    "  1. Add missing pieces to `./lib/fast_rcnn/train.py` (this is where the main training loop is).\n",
    "  2. Implement the network (`./custom/net.py`) and the solver (`./custom/solver.py`) class respecting the interface described in the source files. **You are NOT required to code up the bounding-box regression.** \n",
    "  3. Repeat the steps above for two kinds of training regimes:\n",
    "    - Fine-tuning of the top layer.\n",
    "    - Fine-tuning of the whole network.\n",
    "    \n",
    "### Custom classes\n",
    "  \n",
    "The main purpose of the **`Net`** class is to hold a symbolic representation of your neural network. This is where you load a pretrained NN optionally modifying its architecture or weights.\n",
    " \n",
    "The **`Solver`** is used to define actual training procedure:\n",
    "  1. Create an instance of **`Net`**\n",
    "  2. Define a loss function\n",
    "  3. Define an optimization algorithm and its parameters\n",
    "  4. Gather all of the above into the training step function\n",
    "  \n",
    "### ROIDataLayer\n",
    "\n",
    "Your solver is likely to require some kind of training data source. In the orginial Girshick's code this data is obtained via **`ROIDataLayer`**. The **`ROIDataLayer`** takes care of the dataset management and produces tensors that are ready for the neural net processing. We are providing you with an adapted version of this layer suitable for incorporating into your custom modules. The typical usage is outlined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do NOT run this cell. This serves just as an illustration.\n",
    "\n",
    "from roi_data_layer.layer import RoIDataLayer\n",
    "\n",
    "# Create an instance and run initial setup.\n",
    "roi_data_layer = RoIDataLayer()\n",
    "roi_data_layer.setup()\n",
    "\n",
    "# This is important! You need to supply the roidb. This one is available in ./lib/fast_rcnn/train.py via SolverWrapper (look at the __init__).\n",
    "# By the way, SolverWrapper.__init__ is the place where you should instantiate your solver.\n",
    "roi_data_layer.set_roidb(roidb)\n",
    "\n",
    "# This is what you'd call at every training iteration.\n",
    "# This populates roi_data_layer.top list with with tensors that you might need for conducting a step of you solver.\n",
    "roi_data_layer.forward()\n",
    "# First three tensors are:\n",
    "#   1. data (ndarray): (cfg.TRAIN.IMS_PER_BATCH, 3, image_height, image_width) tensor containing the whole scenes\n",
    "#   2. rois (ndarray): (cfg.TRAIN.BATCH_SIZE, 5) tensor containg ROIs; rois[:, 0] are indices of scenes in data, the rest\n",
    "#                      are (left, top, right, bottom) coordinates\n",
    "#   3. labels (ndarray): (cfg.TRAIN.BATCH_SIZE,) tensor contaning correct labels (those are float32, convert if needed)\n",
    "data, rois, labels = roi_data_layer.top[: 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tips\n",
    "  1. Please **be careful with the input format**. Make sure that your network receives what it expects, i.e. check the order of color channels (`RGB`/`BGR`), range of values ($ [0, 1] $/ $ [0, 255] $), etc.\n",
    "  2. Try Dropout to reduce overfitting.\n",
    "  3. Try different optimization algorithms. `Adam` seems to require less tuning but may give slightly worse results than `SGD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "--------------------\n",
    "\n",
    "Now it's time to train your freshly written model. First, edit `./experiments/cfgs/rcnn.yml` to override the standard settings.\n",
    "\n",
    "The training procedure is launched by invoking the following shell-command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change this to adjust the number of training iterations.\n",
    "NUM_ITERS=40000\n",
    "\n",
    "!./experiments/scripts/train_rcnn.sh {NUM_ITERS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model\n",
    "--------------------\n",
    "\n",
    "Let's asses the quality of trained model on the test set of PASCAL VOC 2007. In order to do this, you have to implement the **`Tester`** encapsulating a trained network. The **`Tester`**'s sole purpose is to produce confidence scores for bounding-boxes at, well, test time.\n",
    "\n",
    "After the coding is done, the evaluation can be run by invoking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put the path to the network snapshot here:\n",
    "SNAPSHOT=''\n",
    "\n",
    "!./experiments/scripts/test_rcnn.sh {SNAPSHOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is rather large (**4952** images) so it may take some time for the evaluation to finish. Just like in any serious ML research, it's a good idea to leave this thing running overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a short report\n",
    "--------------------\n",
    "\n",
    "Describe the model that you use for this assignment. How do you train it? What did work and what didn't? **Most importantly, does fine-tuning of the whole network work any better than just adapting the last layer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If things are slow for you (advanced stuff)\n",
    "--------------------\n",
    "\n",
    "Then you are welcome to replace an older R-CNN pipeline with a newer **Fast R-CNN** approach (see [this paper](http://arxiv.org/abs/1504.08083) and, again, [this presentation](http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf)). The only missing piece is the so-called **ROI Pooling**. Fortunately, you can find a Theano implementation [here](https://github.com/ddtm/theano-roi-pooling) (**it's still somewhat untested**). Roughly speaking, you need to do the following:\n",
    "  1. Wrap the Theano Op into a Lasagne's custom layer (see http://lasagne.readthedocs.org/en/latest/user/custom_layers.html).\n",
    "  2. Change your network to receive two inputs (the whole image(s) and a set of rois - those are available through `ROIDataLayer`) instead of stack of patches of a fixed-size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
